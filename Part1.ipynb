{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gttEcYZych3h"
      },
      "source": [
        "**Installing Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAwmBI8bbJe3"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZSvuJzBc4zd"
      },
      "source": [
        "**Import statements**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "69z-ajlX05I-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xnSuJiIBcv1O"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)"
      ],
      "metadata": {
        "id": "Sl7KYXkRRZBs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_28PzNuacUyW"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "60247c0101774102b5d06194c0c720da",
            "63805a7384874d5ab7f96d65efb7fe82",
            "c2bcabd8eb654abd81f2d587367a06d5",
            "fd29513978c04391868ccfa1433d4a68",
            "1b810ef69e294b1a80292371b8c3177e",
            "6dc7a865a1924bc78d3c25f4342943c0",
            "68142fc562dd4fce866f5a65754a9ce3",
            "f6af2b38810c4a11be2d9f597ce1684e"
          ]
        },
        "id": "foCbEVjGa8tg",
        "outputId": "c02742f5-ed6a-421a-887c-de62f327316b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:3j6i0wkz) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.068419â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60247c0101774102b5d06194c0c720da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Assignment1_sample_images</strong> at: <a href='https://wandb.ai/cs22m006/Assignment%201/runs/3j6i0wkz' target=\"_blank\">https://wandb.ai/cs22m006/Assignment%201/runs/3j6i0wkz</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230307_062732-3j6i0wkz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:3j6i0wkz). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230307_062823-vo7dpwmh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m006/Assignment%201/runs/vo7dpwmh' target=\"_blank\">Assignment1_sample_images</a></strong> to <a href='https://wandb.ai/cs22m006/Assignment%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m006/Assignment%201' target=\"_blank\">https://wandb.ai/cs22m006/Assignment%201</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m006/Assignment%201/runs/vo7dpwmh' target=\"_blank\">https://wandb.ai/cs22m006/Assignment%201/runs/vo7dpwmh</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "wandb.init(\n",
        "    project=\"Assignment 1\",\n",
        "    entity=\"cs22m006\",\n",
        "    name=\"Assignment1_sample_images\"\n",
        ")\n",
        "def plotImagesOfEachClass():\n",
        "  image_labels = []\n",
        "  images = []\n",
        "  for i in range(len(trainX)):\n",
        "    if len(image_labels) == len(class_names):\n",
        "      break\n",
        "    if class_names[trainy[i]] not in image_labels:\n",
        "      image_labels.append(class_names[trainy[i]])\n",
        "      images.append(trainX[i])\n",
        "\n",
        "  wandb.log({\"Sample image for each class \": [wandb.Image(img, caption=caption) for img, caption in zip(images, image_labels)]})\n",
        "\n",
        "plotImagesOfEachClass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rUN0fivd4Yg"
      },
      "source": [
        "# **Question 2 and 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVxycC5wmAo_"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (testX, testy) = fashion_mnist.load_data()\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
        "x_train = x_train/255.0\n",
        "testX = testX.reshape(testX.shape[0], testX.shape[1]*testX.shape[2])\n",
        "testX = testX/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4KVDIokWeG9o"
      },
      "outputs": [],
      "source": [
        "def initializeWeightAndBias(layer_dims, init_mode = \"random uniform\"):\n",
        "  W = []\n",
        "  bias = []\n",
        "  np.random.seed(3)\n",
        "  if(init_mode == \"random uniform\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1], layer_dims[layer_num])))\n",
        "      bias.append((np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1],1))))\n",
        "  elif(init_mode == \"random normal\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1], layer_dims[layer_num]))\n",
        "      bias.append((np.random.randn(layer_dims[layer_num+1],1)))\n",
        "  elif(init_mode == \"xavier\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1],layer_dims[layer_num])*np.sqrt(2/(layer_dims[layer_num+1]+layer_dims[layer_num])))\n",
        "      bias.append(np.random.randn(layer_dims[layer_num+1],1)*np.sqrt(2/(layer_dims[layer_num+1])))\n",
        "  return W, bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feedForward(W, bias, X, num_hidden_layers, layer_dims, activation_fun = \"tanh\"):\n",
        "  preactivation = []\n",
        "  activation = []\n",
        "  activation.append(X.T)\n",
        "  preactivation.append(X.T)\n",
        "  for i in range(1, num_hidden_layers+1):\n",
        "    preactivation.append(bias[i-1] + np.matmul(W[i-1], activation[(i-1)]))\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      activation.append(sigmoid(preactivation[i]))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      activation.append(tanh(preactivation[i]))\n",
        "    elif(activation_fun == \"reLU\"):\n",
        "      activation.append(reLU(preactivation[i]))\n",
        "  preactivation.append(bias[-1] + np.dot(W[-1], activation[-1]))\n",
        "  activation.append(softmax(preactivation[-1]))\n",
        "  return activation[-1], activation, preactivation"
      ],
      "metadata": {
        "id": "H4l-gojV1ctE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QVmdL1b84nT3"
      },
      "outputs": [],
      "source": [
        "def updateParam(W, gradientW, bias, gradientBias, learning_rate):\n",
        "  for i in range(0, len(W)):\n",
        "    W[i] = W[i] - learning_rate*gradientW[i]\n",
        "    bias[i] = bias[i] - learning_rate*gradientBias[i]\n",
        "  return W, bias\n",
        "\n",
        "def updateParamMomentum(W, bias, gradientW, gradientBias, previous_updates_W, previous_updates_Bias, learning_rate, momentum):\n",
        "  for idx in range(len(gradientW)):\n",
        "    previous_updates_W[idx] = momentum*previous_updates_W[idx] + gradientW[idx]\n",
        "    previous_updates_Bias[idx] = momentum*previous_updates_Bias[idx] + gradientBias[idx]\n",
        "  for i in range(0, len(W)):\n",
        "    W[i] = W[i] - learning_rate*gradientW[i]\n",
        "    bias[i] = bias[i] - learning_rate*gradientBias[i]\n",
        "  return W, bias\n",
        "  \n",
        "\n",
        "def updateParamRMS(W, gradientW, bias, gradientBias, learning_rate, v_W, v_bias, beta):\n",
        "  eps = 1e-6\n",
        "  for idx in range(0, len(W)):\n",
        "    v_W_t = beta*v_W[idx] + (1-beta)*np.multiply(gradientW[idx], gradientW[idx])\n",
        "    v_bias_t = beta*v_bias[idx] + (1-beta)*np.multiply(gradientBias[idx], gradientBias[idx])\n",
        "    W[idx] = W[idx] - learning_rate*gradientW[idx]/(np.sqrt(v_W_t)+eps)\n",
        "    bias[idx] = bias[idx] - learning_rate*gradientBias[idx]/(np.sqrt(v_bias_t)+eps)\n",
        "    v_W[idx] = v_W_t\n",
        "    v_bias[idx] = v_bias_t\n",
        "  return W, bias, v_W, v_bias\n",
        "\n",
        "def updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2):\n",
        "\n",
        "  epsilon = 1e-6\n",
        "\n",
        "  for i in range(0, len(W)):\n",
        "    mdW = beta1*m_W[i] + (1-beta1)*gradientW[i]\n",
        "    mdBias = beta1*m_bias[i] + (1-beta1)*gradientBias[i]\n",
        "    vdW = beta2*v_W[i] + (1-beta2)*np.square(gradientW[i])\n",
        "    vdBias = beta2*v_bias[i] + (1-beta2)*np.square(gradientBias[i])\n",
        "    m_w_hat = mdW/(1.0 - beta1**t)\n",
        "    v_w_hat = vdW/(1.0 - beta2**t)\n",
        "    m_bias_hat = mdBias/(1.0 - beta1**t)\n",
        "    v_bias_hat = vdBias/(1.0 - beta2**t)\n",
        "\n",
        "    W[i] = W[i] - (learning_rate * m_w_hat)/np.sqrt(v_w_hat + epsilon)\n",
        "    bias[i] = bias[i] - (learning_rate * m_bias_hat)/np.sqrt(v_bias_hat + epsilon)\n",
        "\n",
        "    v_W[i] = vdW\n",
        "    m_W[i] = mdW\n",
        "    v_bias[i] = vdBias\n",
        "    m_bias[i] = mdBias\n",
        "\n",
        "    return W, bias, v_W, v_bias, m_W, m_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qRCs-OZUPD3W"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "  return 1.0/(1.+np.exp(-X))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def reLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def reLU_derivative(x):\n",
        "  return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(a):\n",
        "  return np.exp(a)/np.sum(np.exp(a), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateAccuracy(batch_size, X, y, W, bias, num_hidden_layers, layer_dims, activation_fun):\n",
        "  batch_count = batch_size\n",
        "  count = 0\n",
        "  for i in range(0, len(X), batch_size):\n",
        "    if(i+batch_size>len(X)):\n",
        "      batch_count = len(X)-i\n",
        "    hL, activation, preactivation = feedForward(W, bias, X[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "    for j in range(i, i+batch_count):\n",
        "      if(np.argmax(hL[:,(j-i)]) == y[j]):\n",
        "        count+=1\n",
        "  return (100.0*count)/len(X)"
      ],
      "metadata": {
        "id": "YrgTxGepC5sh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "meGdgBul6FQJ"
      },
      "outputs": [],
      "source": [
        "def backward_propogation(y_one_hot, x, y, W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun = \"tanh\"):\n",
        "  L = num_hidden_layers+1\n",
        "  gradientPreactivation = []\n",
        "  gradientPreactivation.append(activation[L]-y_one_hot)\n",
        "  gradientWeight = []\n",
        "  gradientBias = []\n",
        "  for k in range(L, 0, -1):\n",
        "    gradientWeight.append(np.matmul(gradientPreactivation[-1], activation[k-1].T)/batch_size)\n",
        "    gradientBias.append(np.sum(gradientPreactivation[-1], axis=1, keepdims=True)/batch_size)\n",
        "    if k==1:\n",
        "      break\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), sigmoid_derivative(preactivation[k-1])))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), tanh_derivative(preactivation[k-1])))\n",
        "    if(activation_fun == \"reLU\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), reLU_derivative(preactivation[k-1])))\n",
        "  return gradientWeight[::-1], gradientBias[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lvgQ1EXUJgzX"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y, y_hat, W, weight_decay):\n",
        "  loss = 0\n",
        "  for i in range(len(y)):\n",
        "    loss += -1.0*np.sum(y[i]*np.log(y_hat[i]))\n",
        "  \n",
        "  #L2 regularizaation\n",
        "  acc = 0\n",
        "  for i in range(len(W)):\n",
        "    acc += np.sum(W[i]**2)\n",
        "  loss += weight_decay*acc\n",
        "  return loss\n",
        "\n",
        "def mse(y, y_hat, W, weight_decay):\n",
        "  loss = 1/2* np.sum((y-y_hat)**2)\n",
        "  #L2 regularizaation\n",
        "  acc = 0\n",
        "  for i in range(len(W)):\n",
        "    acc += np.sum(W[i]**2)\n",
        "  loss += weight_decay*acc\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-T4dE82QSVGS"
      },
      "outputs": [],
      "source": [
        "def optimizers(num_hidden_layers, neurons_in_each_layer, epochs, learning_rate, batch_size, init_mode, activation_fun, loss_function = \"cross_entropy\", optimizer = \"sgd\", momentum = 0.9, beta = 0.9, beta1 = 0.9, beta2 = 0.999, weight_decay = 0):\n",
        "  layer_dims = [trainX.shape[1]]\n",
        "  for i in range(num_hidden_layers):\n",
        "    layer_dims.append(neurons_in_each_layer)\n",
        "  layer_dims.append(k)\n",
        "  W, bias = initializeWeightAndBias(layer_dims, init_mode)\n",
        "  y_pred = []\n",
        "  batch_count = batch_size\n",
        "  y_one_hot = np.zeros((10, num_images))\n",
        "  for i in range(num_images):\n",
        "    y_one_hot[trainy[i]][i] = 1\n",
        "  v_W = [0]*(num_hidden_layers+1)\n",
        "  v_bias = [0]*(num_hidden_layers+1)\n",
        "  m_W, m_bias, gradientW, gradientBias, look_ahead_W, look_ahead_bias, previous_updates_W, previous_updates_Bias = v_W.copy(), v_bias.copy(), v_W.copy(), v_bias.copy(), v_W.copy(), v_bias.copy(), v_W.copy(), v_bias.copy()\n",
        "  t = 1 #for adam\n",
        "\n",
        "  for iterationNumber in range(epochs):\n",
        "    loss=0\n",
        "    batch_count = batch_size\n",
        "    for i in range(0, num_images, batch_size):\n",
        "      if(i+batch_size >= num_images):\n",
        "        batch_count = num_images-i\n",
        "\n",
        "      if(optimizer == \"nag\"):\n",
        "        for idx in range(len(W)):\n",
        "          look_ahead_W[idx] = W[idx] - momentum * gradientW[idx]\n",
        "          look_ahead_bias[idx] = bias[idx] - momentum * gradientBias[idx]\n",
        "\n",
        "        hL, activation, preactivation = feedForward(look_ahead_W, look_ahead_bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], look_ahead_W, look_ahead_bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun)\n",
        "        W, bias = updateParam(W, gradientW, bias, gradientBias, learning_rate)\n",
        "\n",
        "      elif(optimizer == \"nadam\"):\n",
        "        for idx in range(len(W)):\n",
        "          look_ahead_W[idx] = W[idx] - momentum * gradientW[idx]\n",
        "          look_ahead_bias[idx] = bias[idx] - momentum * gradientBias[idx]\n",
        "\n",
        "        hL, activation, preactivation = feedForward(look_ahead_W, look_ahead_bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], look_ahead_W, look_ahead_bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun)\n",
        "        W, bias, v_W, v_bias, m_W, m_bias = updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2)\n",
        "        t += 1\n",
        "\n",
        "      else:\n",
        "        hL, activation, preactivation = feedForward(W, bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "\n",
        "        gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun)\n",
        "  \n",
        "        if(optimizer == \"sgd\"):\n",
        "          W, bias = updateParam(W, gradientW, bias, gradientBias, learning_rate)\n",
        "\n",
        "        elif(optimizer == \"momentum\"):\n",
        "          W, bias = updateParamMomentum(W, bias, gradientW, gradientBias, previous_updates_W, previous_updates_Bias, learning_rate, momentum)\n",
        "        \n",
        "        elif(optimizer == \"rmsprop\"):\n",
        "          W, bias, v_W, v_bias = updateParamRMS(W, gradientW, bias, gradientBias, learning_rate, v_W, v_bias, beta)\n",
        "\n",
        "        elif(optimizer == \"adam\"):\n",
        "          W, bias, v_W, v_bias, m_W, m_bias = updateParamAdam(W, bias, gradientW, gradientBias, v_W, v_bias, m_W, m_bias, t, learning_rate, beta1, beta2)\n",
        "          t += 1\n",
        "\n",
        "      if(iterationNumber==epochs-1):\n",
        "        for j in range(i, i+batch_count):\n",
        "          y_pred.append(np.argmax(hL[:,(j-i)]))\n",
        "      if(loss_function == \"cross_entropy\"):\n",
        "        loss += cross_entropy(y_one_hot[:,i:i+batch_count], hL[:,0:batch_count], W, weight_decay)\n",
        "      elif(loss_function == \"mean_squared_error\"):\n",
        "        loss += mse(y_one_hot[:,i:i+batch_count], hL[:,0:batch_count], W, weight_decay)\n",
        "    \n",
        "    valid_acc = calculateAccuracy(batch_size, validationX, validationy, W, bias, num_hidden_layers, layer_dims, activation_fun)\n",
        "    print(\"validation accuracy at iteration\", (iterationNumber+1), \"=\", valid_acc)\n",
        "    print(\"loss at iteration\", (iterationNumber+1), \"=\", loss/(num_images))\n",
        "  return y_pred, W, bias\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn1arCMJTH8C"
      },
      "outputs": [],
      "source": [
        "num_hidden_layers = 5\n",
        "batch_size = 32\n",
        "k = len(class_names)\n",
        "trainX, validationX, trainy, validationy = train_test_split(x_train, y_train, random_state=104, test_size=0.1, shuffle=True)\n",
        "num_images = len(trainy)\n",
        "image_size = trainX.shape[1]\n",
        "neurons_in_each_layer = 32\n",
        "pred_label, W, bias=optimizers(num_hidden_layers, neurons_in_each_layer, epochs = 10, learning_rate = 0.1, batch_size = 32, init_mode = \"random uniform\", activation_fun = \"sigmoid\", loss_function = \"cross_entropy\", optimizer = \"sgd\", momentum = 0.9, beta = 0.9, beta1 = 0.9, beta2 = 0.999, weight_decay = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fXqPcIYQNxvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e6730d-0821-4932-a8d7-4d7f89a36466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data 81.11296296296297\n"
          ]
        }
      ],
      "source": [
        "cnt=0\n",
        "for i in range(len(pred_label)):\n",
        "  if(pred_label[i]==trainy[i]):\n",
        "    cnt+=1\n",
        "print(\"Accuracy on train data\", 100*cnt/len(pred_label))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [trainX.shape[1]]\n",
        "for i in range(num_hidden_layers):\n",
        "  layer_dims.append(neurons_in_each_layer)\n",
        "layer_dims.append(k)"
      ],
      "metadata": {
        "id": "QAihAxgpt5CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_count = batch_size\n",
        "count = 0\n",
        "for i in range(0, len(validationX), batch_size):\n",
        "  if(i+batch_size>len(validationX)):\n",
        "    batch_count = len(validationX)-i-1\n",
        "  hL, activation, preactivation = feedForward(W, bias, validationX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun = \"sigmoid\")\n",
        "  for j in range(i, i+batch_count):\n",
        "    if(np.argmax(hL[:,(j-i)]) == validationy[j]):\n",
        "      count+=1\n",
        "print(\"Accuracy on validation data\", (100.0*count)/len(validationX))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpT3iFmh1rRV",
        "outputId": "e230df8e-aa8b-47a6-cd83-916d69dacfdf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation data 81.66666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb6eMz2QXbIF",
        "outputId": "30ef2ac6-1907-45ab-96f7-5000ec0b93a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data 81.2\n"
          ]
        }
      ],
      "source": [
        "batch_count = batch_size\n",
        "count = 0\n",
        "for i in range(0, len(testX), batch_size):\n",
        "  if(i+batch_size>len(testX)):\n",
        "    batch_count = len(testX)-i-1\n",
        "  hL, activation, preactivation = feedForward(W, bias, testX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun = \"sigmoid\")\n",
        "  for j in range(i, i+batch_count):\n",
        "    if(np.argmax(hL[:,(j-i)]) == testy[j]):\n",
        "      count+=1\n",
        "print(\"Accuracy on test data\", (100.0*count)/len(testX))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60247c0101774102b5d06194c0c720da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63805a7384874d5ab7f96d65efb7fe82",
              "IPY_MODEL_c2bcabd8eb654abd81f2d587367a06d5"
            ],
            "layout": "IPY_MODEL_fd29513978c04391868ccfa1433d4a68"
          }
        },
        "63805a7384874d5ab7f96d65efb7fe82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b810ef69e294b1a80292371b8c3177e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6dc7a865a1924bc78d3c25f4342943c0",
            "value": "0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c2bcabd8eb654abd81f2d587367a06d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68142fc562dd4fce866f5a65754a9ce3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6af2b38810c4a11be2d9f597ce1684e",
            "value": 0.06841982778682802
          }
        },
        "fd29513978c04391868ccfa1433d4a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b810ef69e294b1a80292371b8c3177e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc7a865a1924bc78d3c25f4342943c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68142fc562dd4fce866f5a65754a9ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6af2b38810c4a11be2d9f597ce1684e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}