{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gttEcYZych3h"
      },
      "source": [
        "**Installing Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAwmBI8bbJe3"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZSvuJzBc4zd"
      },
      "source": [
        "**Import statements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xnSuJiIBcv1O"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiZOfP0Yrygd"
      },
      "source": [
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DKylrRlrxG0"
      },
      "outputs": [],
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)"
      ],
      "metadata": {
        "id": "Sl7KYXkRRZBs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_28PzNuacUyW"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foCbEVjGa8tg"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"Assignment 1\",\n",
        ")\n",
        "def plotImagesOfEachClass():\n",
        "  image_labels = []\n",
        "  images = []\n",
        "  for i in range(len(trainX)):\n",
        "    if len(image_labels) == len(class_names):\n",
        "      break\n",
        "    if class_names[trainy[i]] not in image_labels:\n",
        "      image_labels.append(class_names[trainy[i]])\n",
        "      images.append(trainX[i])\n",
        "\n",
        "  wandb.log({\"examples \": [wandb.Image(img, caption=caption) for img, caption in zip(images, image_labels)]})\n",
        "\n",
        "plotImagesOfEachClass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rUN0fivd4Yg"
      },
      "source": [
        "# **Question 2 and 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UVxycC5wmAo_"
      },
      "outputs": [],
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "num_images = len(trainX)\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)\n",
        "trainX = trainX.reshape(trainX.shape[0], trainX.shape[1]*trainX.shape[2])\n",
        "trainX = trainX/255.0\n",
        "testX = testX.reshape(testX.shape[0], testX.shape[1]*testX.shape[2])\n",
        "testX = testX/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "4KVDIokWeG9o"
      },
      "outputs": [],
      "source": [
        "def initializeWeightAndBias(layer_dims, init_mode = \"random uniform\"):\n",
        "  W = []\n",
        "  bias = []\n",
        "  np.random.seed(3)\n",
        "  if(init_mode == \"random uniform\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1], layer_dims[layer_num])))\n",
        "      bias.append((np.random.uniform(-0.7, 0.7, (layer_dims[layer_num+1],1))))\n",
        "  elif(init_mode == \"random normal\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1], layer_dims[layer_num]))\n",
        "      bias.append((np.random.randn(layer_dims[layer_num+1],1)))\n",
        "  elif(init_mode == \"xavier\"):\n",
        "    for layer_num in range(len(layer_dims)-1):\n",
        "      W.append(np.random.randn(layer_dims[layer_num+1],layer_dims[layer_num])*np.sqrt(2/(layer_dims[layer_num+1]+layer_dims[layer_num])))\n",
        "      bias.append(np.random.randn(layer_dims[layer_num+1],1)*np.sqrt(2/(layer_dims[layer_num+1])))\n",
        "  return W, bias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feedForward(W, bias, X, num_hidden_layers, layer_dims, activation_fun = \"tanh\"):\n",
        "  preactivation = []\n",
        "  activation = []\n",
        "  activation.append(X.T)\n",
        "  preactivation.append(X.T)\n",
        "  for i in range(1, num_hidden_layers+1):\n",
        "    preactivation.append(bias[i-1] + np.matmul(W[i-1], activation[(i-1)]))\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      activation.append(sigmoid(preactivation[i]))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      activation.append(tanh(preactivation[i]))\n",
        "    elif(activation_fun == \"reLU\"):\n",
        "      activation.append(reLU(preactivation[i]))\n",
        "  preactivation.append(bias[-1] + np.dot(W[-1], activation[-1]))\n",
        "  activation.append(softmax(preactivation[-1]))\n",
        "  return activation[-1], activation, preactivation"
      ],
      "metadata": {
        "id": "H4l-gojV1ctE"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "QVmdL1b84nT3"
      },
      "outputs": [],
      "source": [
        "def updateParam(W, gradientW, bias, gradientBias, learning_rate):\n",
        "  for i in range(0, len(W)):\n",
        "    W[i] = W[i] - learning_rate*gradientW[i]\n",
        "    bias[i] = bias[i] - learning_rate*gradientBias[i]\n",
        "  return W, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "qRCs-OZUPD3W"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "  return 1.0/(1.+np.exp(-X))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def reLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def reLU_derivative(x):\n",
        "  return 1*(x>0) \n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "  return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(a):\n",
        "  return np.exp(a)/np.sum(np.exp(a), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "meGdgBul6FQJ"
      },
      "outputs": [],
      "source": [
        "def backward_propogation(y_one_hot, x, y, W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun = \"tanh\"):\n",
        "  L = num_hidden_layers+1\n",
        "  gradientPreactivation = []\n",
        "  gradientPreactivation.append(activation[L]-y_one_hot)\n",
        "  gradientWeight = []\n",
        "  gradientBias = []\n",
        "  for k in range(L, 0, -1):\n",
        "    gradientWeight.append(np.matmul(gradientPreactivation[-1], activation[k-1].T)/batch_size)\n",
        "    gradientBias.append(np.sum(gradientPreactivation[-1], axis=1, keepdims=True)/batch_size)\n",
        "    if k==1:\n",
        "      break\n",
        "    if(activation_fun == \"sigmoid\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), sigmoid_derivative(preactivation[k-1])))\n",
        "    elif(activation_fun == \"tanh\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), tanh_derivative(preactivation[k-1])))\n",
        "    if(activation_fun == \"reLU\"):\n",
        "      gradientPreactivation.append(np.multiply(np.matmul(W[k-1].T, gradientPreactivation[-1]), reLU_derivative(preactivation[k-1])))\n",
        "  return gradientWeight[::-1], gradientBias[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "lvgQ1EXUJgzX"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y, y_hat):\n",
        "  loss = 0\n",
        "  for i in range(len(y)):\n",
        "    loss += -1.0*np.sum(y[i]*np.log(y_hat[i]))\n",
        "  return loss\n",
        "\n",
        "def mse(y, y_hat):\n",
        "  return 1/2* np.sum((y-y_hat)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "-T4dE82QSVGS"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(num_hidden_layers, layer_dims, epochs, learning_rate, batch_size, init_mode, activation_fun, loss_function = \"cross_entropy\", optimizer = \"sgd\", beta = 0.9):\n",
        "  W, bias = initializeWeightAndBias(layer_dims, init_mode)\n",
        "  y_pred = []\n",
        "  batch_count = batch_size\n",
        "  y_one_hot = np.zeros((10, num_images))\n",
        "  for i in range(num_images):\n",
        "    y_one_hot[trainy[i]][i] = 1\n",
        "\n",
        "  for iterationNumber in range(epochs):\n",
        "    loss=0\n",
        "    for i in range(0, num_images, batch_size):\n",
        "      if(i+batch_size > num_images):\n",
        "        batch_count = num_images-i-1\n",
        "\n",
        "      hL, activation, preactivation = feedForward(W, bias, trainX[i:i+batch_count], num_hidden_layers, layer_dims, activation_fun)\n",
        "\n",
        "      if(loss_function == \"cross_entropy\"):\n",
        "        loss += cross_entropy(y_one_hot[:,i:i+batch_count], hL[:,0:batch_count])\n",
        "      elif(loss_function == \"mean_squared_error\"):\n",
        "        loss += mse(y_one_hot[:,i:i+batch_count], hL[:,0:batch_count])\n",
        "\n",
        "      gradientW, gradientBias = backward_propogation(y_one_hot[:,i:i+batch_count], trainX[i:i+batch_count], trainy[i:i+batch_count], W, bias, activation, preactivation, num_hidden_layers, batch_size, activation_fun)\n",
        "      if(iterationNumber==epochs-1):\n",
        "        for j in range(i, i+batch_count):\n",
        "          y_pred.append(np.argmax(hL[:,(j-i)]))\n",
        "\n",
        "      if(optimizer == \"sgd\"):\n",
        "        W, bias = updateParam(W, gradientW, bias, gradientBias, learning_rate)\n",
        "\n",
        "      elif(optimizer == \"momentum\"):\n",
        "        \n",
        "        if(i==0 and iterationNumber == 0):\n",
        "          previous_updates_W = gradientW\n",
        "          previous_updates_Bias = gradientBias\n",
        "        else:\n",
        "          for idx in range(len(gradientW)):\n",
        "            previous_updates_W[idx] = beta*previous_updates_W[idx] + gradientW[idx]\n",
        "            previous_updates_Bias[idx] = beta*previous_updates_Bias[idx] + gradientBias[idx]\n",
        "        print(i, len(gradientW), len(gradientBias), len(previous_updates_W), len(previous_updates_Bias), gradientBias[0].shape, previous_updates_Bias[0].shape)\n",
        "        W, bias = updateParam(W, previous_updates_W, bias, previous_updates_Bias, learning_rate)\n",
        "\n",
        "    print(\"loss at iteration\", (iterationNumber+1), \"=\", loss/(num_images))\n",
        "  return y_pred, W, bias\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "mn1arCMJTH8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4924ad56-251f-42f1-e947-6ab6c80908f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at iteration 1 = 1.2305062080793658\n",
            "loss at iteration 2 = 0.602596792798347\n",
            "loss at iteration 3 = 0.5115135463691484\n",
            "loss at iteration 4 = 0.46887541739006594\n",
            "loss at iteration 5 = 0.44180074324818813\n",
            "loss at iteration 6 = 0.42221693113918535\n",
            "loss at iteration 7 = 0.40690917231035767\n",
            "loss at iteration 8 = 0.3943298859098584\n",
            "loss at iteration 9 = 0.38363033232635024\n",
            "loss at iteration 10 = 0.37430452013329585\n"
          ]
        }
      ],
      "source": [
        "num_hidden_layers = 5\n",
        "neurons_in_each_layer = 128\n",
        "batch_size = 32\n",
        "k = len(class_names)\n",
        "num_images = len(trainX)\n",
        "image_size = trainX.shape[1]\n",
        "layer_dims = [image_size]\n",
        "random.seed(3)\n",
        "for l in range(num_hidden_layers):\n",
        "  layer_dims.append(random.randint(10,100))\n",
        "layer_dims.append(k)\n",
        "pred_label, W, bias=stochastic_gradient_descent(num_hidden_layers, layer_dims, epochs = 10, learning_rate = 0.1, batch_size = 32, init_mode = \"random uniform\", activation_fun = \"sigmoid\", loss_function = \"cross_entropy\", optimizer = \"momentum\", beta = 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "fXqPcIYQNxvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e20205-d6ce-49b2-9e27-4743dc09cb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data 86.49833333333333\n"
          ]
        }
      ],
      "source": [
        "cnt=0\n",
        "for i in range(len(pred_label)):\n",
        "  if(pred_label[i]==trainy[i]):\n",
        "    cnt+=1\n",
        "print(\"Accuracy on train data\", 100*cnt/len(pred_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb6eMz2QXbIF",
        "outputId": "018f81d3-4a4f-47b3-9059-44934e2569c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data 84.73\n"
          ]
        }
      ],
      "source": [
        "batch_count = batch_size\n",
        "count = 0\n",
        "for i in range(0, len(testX), batch_size):\n",
        "  if(i+batch_size>len(testX)):\n",
        "    batch_count = len(testX)-i-1\n",
        "  hL, activation, preactivation = feedForward(W, bias, testX[i:i+batch_count], num_hidden_layers, neurons_in_each_layer, activation_fun = \"sigmoid\")\n",
        "  for j in range(i, i+batch_count):\n",
        "    if(np.argmax(hL[:,(j-i)]) == testy[j]):\n",
        "      count+=1\n",
        "print(\"Accuracy on test data\", (100.0*count)/len(testX))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}